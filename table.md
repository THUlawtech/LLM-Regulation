| Title | Type | Country/Region | Date | Organization | Source | Risks | Risk Perspective |
|-------|------|----------------|------|--------------|--------|-------|------------------|
| [[Interim Measures for the Administration of Generative Artificial Intelligence Services of the People's Republic of China](data/Interim Measures for the Administration of Generative Artificial Intelligence Services of the People's Republic of China.pdf)] | 法律法规 - 已生效 | 🇨🇳 中国 | 2023-08-15 00:00:00 | 国家互联网信息办公室等七部门 | https://www.cac.gov.cn/2023-07/13/c_1690898327029107.htm | 3. 训练数据 - 数据标注, 2. 训练数据 - 违规收集 - 个人信息相关, 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为） | AI 风险来源 |
| [[Practice Guidelines for Cybersecurity Standards - Identification Methods for Contents of Generative Artificial Intelligence Services](data/Practice Guidelines for Cybersecurity Standards - Identification Methods for Contents of Generative Artificial Intelligence Services.pdf)] | 标准指南、技术性文件 | 🇨🇳 中国 | 2024-08-25 00:00:00 | TC260 | https://www.tc260.org.cn/front/postDetail.html?id=20230825190345 | 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为） | AI 风险来源 |
| [[TC260 - 003 Basic Security Requirements for Generative Artificial Intelligence Service](data/TC260 - 003 Basic Security Requirements for Generative Artificial Intelligence Service.pdf)] | 标准指南、技术性文件 | 🇨🇳 中国 | 2024-03-01 00:00:00 | TC260 | https://www.tc260.org.cn/front/postDetail.html?id=20240301164054 | 9. 模型 - 准确性、可靠性（幻觉）, 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 3. 训练数据 - 数据标注, 14. 生成内容 - 模型幻觉, 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全）, 18. 供应链相关风险, 2. 训练数据 - 违规收集 - 个人信息相关 | AI 风险来源, AI 生命周期 |
| [[Artificial intelligence—Code of practice for data labeling of machine learning](data/Artificial intelligence—Code of practice for data labeling of machine learning.pdf)] | 标准指南、技术性文件 | 🇨🇳 中国 | 2023-12-01 00:00:00 | TC28 | https://std.samr.gov.cn/gb/search/gbDetailed?id=FC816D04FEB462EBE05397BE0A0AD5FA | 3. 训练数据 - 数据标注 | AI 风险来源 |
| [[harmonised rules and regulations on artificial intelligence ](data/harmonised rules and regulations on artificial intelligence .pdf)] | 法律法规 - 已生效 | 🇪🇺 欧盟 | 2024-06-13 00:00:00 | EU | https://eur-lex.europa.eu/eli/reg/2024/1689/oj | 9. 模型 - 准确性、可靠性（幻觉）, 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全） | AI 风险来源 |
| [[Cyber security risks to artificial intelligence](data/Cyber security risks to artificial intelligence.pdf)] | 研究报告 | 🇬🇧 英国 | 2024-05-15 00:00:00 | Department for
Science, Innovation
& Technology | https://www.gov.uk/government/publications/research-on-the-cyber-security-of-ai/cyber-security-risks-to-artificial-intelligence#methodology | 8. 模型 - 鲁棒性、抗干扰性, 18. 供应链相关风险 | AI 生命周期 |
| [[OWASP Top 10 for LLM Applications 2025](data/OWASP Top 10 for LLM Applications 2025.pdf)] | 研究报告 | 🌍 NONE | 2024-11-18 00:00:00 | OWASP
（开放式 Web 应用程序安全项目，致力于 Web 应用程序安全的国际非营利组织） | https://owasp.org/www-project-top-10-for-large-language-model-applications/ | 5. 训练数据 - 数据泄露, 8. 模型 - 鲁棒性、抗干扰性, 3. 训练数据 - 数据标注, 18. 供应链相关风险, 13. 生成内容 - 数据泄露（包括隐私泄露）, 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为） | AI 生命周期, AI 风险来源 |
| [[Machine learning security principles v2](data/Machine learning security principles v2.pdf)] | 研究报告 | 🇬🇧 英国 | 2024-05-22 00:00:00 | NCSC（英国国家网络安全中心） | https://www.ncsc.gov.uk/blog-post/machine-learning-security-principles-updated | 8. 模型 - 鲁棒性、抗干扰性, 18. 供应链相关风险, 9. 模型 - 准确性、可靠性（幻觉）, 10. 模型 - 被篡改风险（如模型参数被篡改） | AI 生命周期, AI 风险来源 |
| [[Guidelines for Secure AI System Development](data/Guidelines for Secure AI System Development.pdf)] | 标准指南、技术性文件 | 🌍 multi | 2023-11-26 00:00:00 | USA CISA, UK NCSC, the Australian Signals Directorate’s Australian Cyber Security Centre (ASD ACSC), the Canadian Centre for Cyber Security (CCCS), the New Zealand National Cyber Security Centre (NCSC-NZ) | https://www.cisa.gov/news-events/alerts/2023/11/26/cisa-and-uk-ncsc-unveil-joint-guidelines-secure-ai-system-development | 18. 供应链相关风险 | AI 风险来源 |
| [[Secure Software Development Practices for Generative AI and Dual-Use Foundation Models: An SSDF Community Profile](data/Secure Software Development Practices for Generative AI and Dual-Use Foundation Models: An SSDF Community Profile.pdf)] | 标准指南、技术性文件 | 🇺🇸 美国 | 2024-07-26 00:00:00 | NIST | https://csrc.nist.gov/pubs/sp/800/218/a/final | 10. 模型 - 被篡改风险（如模型参数被篡改）, 8. 模型 - 鲁棒性、抗干扰性, 18. 供应链相关风险 | AI 生命周期 |
| [[Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile](data/Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile.pdf)] | 标准指南、技术性文件 | 🇺🇸 美国 | 2024-07-26 00:00:00 | NIST | https://www.nist.gov/publications/artificial-intelligence-risk-management-framework-generative-artificial-intelligence | 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全）, 1. 训练数据 - 违规收集 - 知产相关, 2. 训练数据 - 违规收集 - 个人信息相关, 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 16. 生成内容 - 自身不利影响（如个人认知、社会问题、国家问题） | AI 风险来源 |
| [[Artificial Intelligence Risk Management Framework (AI RMF 1.0)](data/Artificial Intelligence Risk Management Framework (AI RMF 1.0).pdf)] | 标准指南、技术性文件 | 🇺🇸 美国 | 2023-01-26 00:00:00 | NIST | https://www.nist.gov/publications/artificial-intelligence-risk-management-framework-ai-rmf-10 | 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 6. 模型 - 可解释性 , 9. 模型 - 准确性、可靠性（幻觉）, 8. 模型 - 鲁棒性、抗干扰性, 13. 生成内容 - 数据泄露（包括隐私泄露） | AI 风险来源, AI 生命周期 |
| [[Reducing Risks Posed by Synthetic Content](data/Reducing Risks Posed by Synthetic Content.pdf)] | 标准指南、技术性文件 | 🇺🇸 美国 | 2024-11-24 00:00:00 | NIST | https://www.nist.gov/publications/reducing-risks-posed-synthetic-content-overview-technical-approaches-digital-content | 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为）, 16. 生成内容 - 自身不利影响（如个人认知、社会问题、国家问题）, 3. 训练数据 - 数据标注 | AI 风险来源 |
| [[LLM AI Cybersecurity & Governance Checklist](data/LLM AI Cybersecurity & Governance Checklist.pdf)] | 研究报告 | 🌍 NONE | 2024-02-19 00:00:00 | OWASP | https://owasp.org/www-project-top-10-for-large-language-model-applications/llm-top-10-governance-doc/LLM_AI_Security_and_Governance_Checklist-v1.pdf | 2. 训练数据 - 违规收集 - 个人信息相关, 5. 训练数据 - 数据泄露 | AI 风险来源 |
| [[The Bletchley Declaration](data/The Bletchley Declaration.pdf)] | 论坛会议 | 🌍 multi | 2024-11-01 00:00:00 | AI Safety Summit 2023 | https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration | 10. 模型 - 被篡改风险（如模型参数被篡改）, 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为） | AI 风险来源 |
| [[Consensus Statement on Red Lines in Artificial Intelligence](data/Consensus Statement on Red Lines in Artificial Intelligence.pdf)] | 论坛会议 | 🌍 multi | 2024-03-10 00:00:00 | “北京AI安全国际对话”论坛 | https://idais-beijing.baai.ac.cn/?lang=zh | 10. 模型 - 被篡改风险（如模型参数被篡改）, 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为）, 8. 模型 - 鲁棒性、抗干扰性 | AI 风险来源 |
| [[Hiroshima Process International Guiding Principles for Advanced AI system](data/Hiroshima Process International Guiding Principles for Advanced AI system.pdf)] | 标准指南、技术性文件 | 🌍 multi | 2023-10-30 00:00:00 | G7 | https://digital-strategy.ec.europa.eu/en/library/hiroshima-process-international-guiding-principles-advanced-ai-system | 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全）, 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为） | AI 风险来源 |
| [[Hiroshima Process International Code of Conduct for Advanced AI Systems](data/Hiroshima Process International Code of Conduct for Advanced AI Systems.pdf)] | 标准指南、技术性文件 | 🌍 multi | 2023-10-30 00:00:00 | G7 | https://digital-strategy.ec.europa.eu/en/library/hiroshima-process-international-code-conduct-advanced-ai-systems | 1. 训练数据 - 违规收集 - 知产相关, 2. 训练数据 - 违规收集 - 个人信息相关, 13. 生成内容 - 数据泄露（包括隐私泄露）, 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为）, 8. 模型 - 鲁棒性、抗干扰性 | AI 风险来源 |
| [[Blueprint for an AI Bill of Right](data/Blueprint for an AI Bill of Right.pdf)] | 政策性文件 | 🇺🇸 美国 | 2022-10-04 00:00:00 | OSTP (Office of Science and Technology Policy) | https://www.whitehouse.gov/ostp/ai-bill-of-rights/ | 7. 模型 - 公平性（算法涉及层面）, 9. 模型 - 准确性、可靠性（幻觉）, 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全）, 2. 训练数据 - 违规收集 - 个人信息相关, 13. 生成内容 - 数据泄露（包括隐私泄露）, 6. 模型 - 可解释性  | AI 生命周期, AI 风险来源 |
| [[Joint Statement on Enforcement Efforts Against Discrimination and Bias in Automated Systems](data/Joint Statement on Enforcement Efforts Against Discrimination and Bias in Automated Systems.pdf)] | 政策性文件 | 🇺🇸 美国 | 2023-04-25 00:00:00 | Consumer Financial Protection Bureau | https://www.ftc.gov/legal-library/browse/cases-proceedings/public-statements/joint-statement-enforcement-efforts-against-discrimination-bias-automated-systems | 2. 训练数据 - 违规收集 - 个人信息相关, 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 9. 模型 - 准确性、可靠性（幻觉）, 7. 模型 - 公平性（算法涉及层面）, 13. 生成内容 - 数据泄露（包括隐私泄露）, 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为）, 6. 模型 - 可解释性  | AI 生命周期, AI 风险来源 |
| [[Quality Control Standards for Automated Valuation Models](data/Quality Control Standards for Automated Valuation Models.pdf)] | 标准指南、技术性文件 | 🇺🇸 美国 | 2023-06-21 00:00:00 | CFPB, OCC, FRB, FDIC, NCUA, and FHFA | https://www.consumerfinance.gov/rules-policy/final-rules/quality-control-standards-for-automated-valuation-models/ | 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 6. 模型 - 可解释性 , 9. 模型 - 准确性、可靠性（幻觉）, 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为） | AI 生命周期, AI 风险来源 |
| [[Colorado’s Consumer Artificial Intelligence Act (SB 24-205)](data/Colorado’s Consumer Artificial Intelligence Act (SB 24-205).pdf)] | 法律法规 - 已生效 | 🇺🇸 美国 | 2024-05-17 00:00:00 | Colorado | https://leg.colorado.gov/bills/sb24-205 | 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 7. 模型 - 公平性（算法涉及层面）, 6. 模型 - 可解释性 , 5. 训练数据 - 数据泄露, 2. 训练数据 - 违规收集 - 个人信息相关, 13. 生成内容 - 数据泄露（包括隐私泄露） | AI 生命周期, AI 风险来源 |
| [[Safe and Secure Innovation for Frontier Artificial Intelligence Models Act（CA SB 1047）](data/Safe and Secure Innovation for Frontier Artificial Intelligence Models Act（CA SB 1047）.pdf)] | 法律法规 - 制定中 | 🇺🇸 美国 | 2024-09-03 00:00:00 | California | https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202320240SB1047 | 10. 模型 - 被篡改风险（如模型参数被篡改）, 11. 模型 - 缺陷传导（如对有缺陷的模型进行微调，导致问题继续存在）, 8. 模型 - 鲁棒性、抗干扰性, 18. 供应链相关风险, 19. 系统相关风险（api、交互页面等）, 5. 训练数据 - 数据泄露 | AI 生命周期, AI 风险来源 |
| [[Digital Content Provenance Standards (CA AB 3211)](data/Digital Content Provenance Standards (CA AB 3211).pdf)] | 法律法规 - 已生效 | 🇺🇸 美国 | 2024-08-23 00:00:00 | California | https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240AB3211 | 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为）, 18. 供应链相关风险, 3. 训练数据 - 数据标注, 6. 模型 - 可解释性  | AI 生命周期 |
| [[Artificial Intelligence Security Governance Framework v1](data/Artificial Intelligence Security Governance Framework v1.pdf)] | 标准指南、技术性文件 | 🇨🇳 中国 | 2024-09-09 00:00:00 | tc 260 | https://www.tc260.org.cn/front/postDetail.html?id=20240909102807 | 1. 训练数据 - 违规收集 - 知产相关, 2. 训练数据 - 违规收集 - 个人信息相关, 3. 训练数据 - 数据标注, 6. 模型 - 可解释性 , 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 10. 模型 - 被篡改风险（如模型参数被篡改）, 5. 训练数据 - 数据泄露, 17. 算力相关风险, 18. 供应链相关风险, 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为）, 13. 生成内容 - 数据泄露（包括隐私泄露）, 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全） | AI 应用场景, AI 风险来源, AI 生命周期 |
| [[Seizing the Opportunities of Safe, Secure and Trustworthy Artificial Intelligence Systems for Sustainable Development](data/Seizing the Opportunities of Safe, Secure and Trustworthy Artificial Intelligence Systems for Sustainable Development.pdf)] | 政策性文件 | 🌍 NONE | 2024-03-21 00:00:00 | UN | https://digitallibrary.un.org/record/4043244/?v=pdf | 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 5. 训练数据 - 数据泄露, 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全）, 13. 生成内容 - 数据泄露（包括隐私泄露）, 14. 生成内容 - 模型幻觉, 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为）, 6. 模型 - 可解释性 , 7. 模型 - 公平性（算法涉及层面）, 19. 系统相关风险（api、交互页面等）, 17. 算力相关风险 | AI 生命周期, AI 风险来源 |
| [[Guidelines on Securing AI Systems](data/Guidelines on Securing AI Systems.pdf)] | 标准指南、技术性文件 | 🇸🇬 新加坡 | 2024-10-15 00:00:00 | CSA | https://www.csa.gov.sg/Tips-Resource/publications/2024/guidelines-on-securing-ai | 1. 训练数据 - 违规收集 - 知产相关, 2. 训练数据 - 违规收集 - 个人信息相关, 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 6. 模型 - 可解释性 , 7. 模型 - 公平性（算法涉及层面）, 8. 模型 - 鲁棒性、抗干扰性, 9. 模型 - 准确性、可靠性（幻觉）, 10. 模型 - 被篡改风险（如模型参数被篡改）, 13. 生成内容 - 数据泄露（包括隐私泄露）, 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为）, 16. 生成内容 - 自身不利影响（如个人认知、社会问题、国家问题）, 17. 算力相关风险, 18. 供应链相关风险, 19. 系统相关风险（api、交互页面等） | AI 生命周期 |
| [[ICO consultation series on generative AI and data protection](data/ICO consultation series on generative AI and data protection.pdf)] | 研究报告 | 🇬🇧 英国 | 2024-09-18 00:00:00 | ICO | https://ico.org.uk/about-the-ico/ico-and-stakeholder-consultations/ico-consultation-series-on-generative-ai-and-data-protection/ | 13. 生成内容 - 数据泄露（包括隐私泄露）, 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全）, 14. 生成内容 - 模型幻觉, 5. 训练数据 - 数据泄露, 6. 模型 - 可解释性 , 7. 模型 - 公平性（算法涉及层面）, 2. 训练数据 - 违规收集 - 个人信息相关 | AI 风险来源 |
| [[Guidance on AI and data protection](data/Guidance on AI and data protection.pdf)] | 研究报告 | 🇬🇧 英国 | 2023-03-15 00:00:00 | ICO | https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/guidance-on-ai-and-data-protection/ | 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 5. 训练数据 - 数据泄露, 6. 模型 - 可解释性 , 7. 模型 - 公平性（算法涉及层面）, 9. 模型 - 准确性、可靠性（幻觉）, 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全）, 13. 生成内容 - 数据泄露（包括隐私泄露）, 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为）, 17. 算力相关风险, 18. 供应链相关风险, 19. 系统相关风险（api、交互页面等） | AI 风险来源, AI 生命周期 |
| [[Discussion paper on GDPR and LLMs](data/Discussion paper on GDPR and LLMs.pdf)] | 研究报告 | 🇩🇪 德国 | 2024-07-15 00:00:00 | HmbBfDI | https://datenschutz-hamburg.de/news/hamburger-thesen-zum-personenbezug-in-large-language-models | 2. 训练数据 - 违规收集 - 个人信息相关, 9. 模型 - 准确性、可靠性（幻觉）, 13. 生成内容 - 数据泄露（包括隐私泄露）, 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为） | AI 风险来源 |
| [[Checklist for the use of LLM-based chatbots](data/Checklist for the use of LLM-based chatbots.pdf)] | 标准指南、技术性文件 | 🇩🇪 德国 | 2023-11-13 00:00:00 | HmbBfDI | https://datenschutz-hamburg.de/news/checkliste-zum-einsatz-llm-basierter-chatbots | 2. 训练数据 - 违规收集 - 个人信息相关, 13. 生成内容 - 数据泄露（包括隐私泄露）, 9. 模型 - 准确性、可靠性（幻觉）, 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全）, 19. 系统相关风险（api、交互页面等）, 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为） | AI 风险来源 |
| [[Opinion 28/2024 on certain data protection aspects related to the processing of personal data in the context of AI models](data/Opinion 28/2024 on certain data protection aspects related to the processing of personal data in the context of AI models.pdf)] | 研究报告 | 🇪🇺 欧盟 | 2024-10-28 00:00:00 | EDPB | https://www.edpb.europa.eu/our-work-tools/our-documents/opinion-board-art-64/opinion-282024-certain-data-protection-aspects_en | 2. 训练数据 - 违规收集 - 个人信息相关, 5. 训练数据 - 数据泄露, 7. 模型 - 公平性（算法涉及层面）, 8. 模型 - 鲁棒性、抗干扰性, 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全）, 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为） | AI 风险来源 |
| [[The Artificial Intelligence and Data Act (AIDA)](data/The Artificial Intelligence and Data Act (AIDA).pdf)] | 法律法规 - 制定中 | 🇨🇦 加拿大 | 2022-06-16 00:00:00 | Minister of Innovation, Science and Industry | https://www.parl.ca/DocumentViewer/en/44-1/bill/C-27/first-reading | 2. 训练数据 - 违规收集 - 个人信息相关, 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 7. 模型 - 公平性（算法涉及层面）, 8. 模型 - 鲁棒性、抗干扰性, 9. 模型 - 准确性、可靠性（幻觉）, 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全）, 13. 生成内容 - 数据泄露（包括隐私泄露）, 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为）, 19. 系统相关风险（api、交互页面等） | AI 生命周期, AI 风险来源 |
| [[Voluntary  Code of Conduct on the Responsible Development and Management of Advanced  Generative AI Systems](data/Voluntary  Code of Conduct on the Responsible Development and Management of Advanced  Generative AI Systems.pdf)] | 标准指南、技术性文件 | 🇨🇦 加拿大 | 2024-09-01 00:00:00 | nan | https://ised-isde.canada.ca/site/ised/en/voluntary-code-conduct-responsible-development-and-management-advanced-generative-ai-systems | 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全）, 8. 模型 - 鲁棒性、抗干扰性, 9. 模型 - 准确性、可靠性（幻觉）, 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为）, 10. 模型 - 被篡改风险（如模型参数被篡改） | AI 风险来源 |
| [[Australia’s AI Ethics Principles](data/Australia’s AI Ethics Principles.pdf)] | 标准指南、技术性文件 | 🇦🇺 澳大利亚 | 2019-01-01 00:00:00 | Department of Industry, Science and Resources | https://www.industry.gov.au/publications/australias-artificial-intelligence-ethics-principles/australias-ai-ethics-principles | 13. 生成内容 - 数据泄露（包括隐私泄露）, 7. 模型 - 公平性（算法涉及层面）, 9. 模型 - 准确性、可靠性（幻觉）, 8. 模型 - 鲁棒性、抗干扰性, 6. 模型 - 可解释性  | AI 风险来源 |
| [[Safe and Responsible  AI in Australia (Discussion paper)](data/Safe and Responsible  AI in Australia (Discussion paper).pdf)] | 研究报告 | 🇦🇺 澳大利亚 | 2023-06-01 00:00:00 | Department of Industry, Science and Resources | https://consult.industry.gov.au/supporting-responsible-ai | 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 9. 模型 - 准确性、可靠性（幻觉）, 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全）, 14. 生成内容 - 模型幻觉, 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为）, 2. 训练数据 - 违规收集 - 个人信息相关 | AI 风险来源 |
| [[Safe and responsible Al in Australia consultation - Australian Government's interim response](data/Safe and responsible Al in Australia consultation - Australian Government's interim response.pdf)] | 研究报告 | 🇦🇺 澳大利亚 | 2024-01-17 00:00:00 | Department of Industry, Science and Resources | https://consult.industry.gov.au/supporting-responsible-ai | 1. 训练数据 - 违规收集 - 知产相关, 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 6. 模型 - 可解释性 , 7. 模型 - 公平性（算法涉及层面）, 9. 模型 - 准确性、可靠性（幻觉）, 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全）, 14. 生成内容 - 模型幻觉, 16. 生成内容 - 自身不利影响（如个人认知、社会问题、国家问题） | AI 风险来源 |
| [[Select Committee on  Adopting
Artificial Intelligence](data/Select Committee on  Adopting
Artificial Intelligence.pdf)] | 法律法规 - 已生效 | 🇦🇺 澳大利亚 | 2024-03-26 00:00:00 | nan | https://www.aph.gov.au/Parliamentary_Business/Committees/Senate/Adopting_Artificial_Intelligence_AI/AdoptingAI | 1. 训练数据 - 违规收集 - 知产相关, 2. 训练数据 - 违规收集 - 个人信息相关, 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 7. 模型 - 公平性（算法涉及层面）, 9. 模型 - 准确性、可靠性（幻觉）, 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全）, 13. 生成内容 - 数据泄露（包括隐私泄露）, 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为）, 17. 算力相关风险, 19. 系统相关风险（api、交互页面等） | AI 生命周期, AI 应用场景 |
| [[National framework for the assurance of  artificial intelligence in government](data/National framework for the assurance of  artificial intelligence in government.pdf)] | 标准指南、技术性文件 | 🇦🇺 澳大利亚 | 2024-06-21 00:00:00 | Australian, state and territory governments | https://www.finance.gov.au/sites/default/files/2024-06/National-framework-for-the-assurance-of-AI-in-government.pdf | 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 9. 模型 - 准确性、可靠性（幻觉）, 6. 模型 - 可解释性 , 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全）, 18. 供应链相关风险 | AI 生命周期 |
| [[AI Guidelines for Business v1](data/AI Guidelines for Business v1.pdf)] | 标准指南、技术性文件 | 🇯🇵 日本 | 2024-04-19 00:00:00 | METI, Ministry of Economy, Trade and Industry | https://www.meti.go.jp/english/press/2024/0419_002.html | 1. 训练数据 - 违规收集 - 知产相关, 2. 训练数据 - 违规收集 - 个人信息相关, 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 7. 模型 - 公平性（算法涉及层面）, 9. 模型 - 准确性、可靠性（幻觉）, 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全）, 19. 系统相关风险（api、交互页面等）, 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为） | AI 生命周期 |
| [[Methodology for the Risk and Impact Assessment of Artificial Intelligence Systems from the Point of View of Human Rights, Democracy and the Rule of Law (HUDERIA Methodology)



用德语将"HUDERIA METHODOLOGY"改写成首字母大写的表达
除了首字母大写，还有哪些常见的改写文本格式？
怎样确保改写后的文本准确传达原意？](data/Methodology for the Risk and Impact Assessment of Artificial Intelligence Systems from the Point of View of Human Rights, Democracy and the Rule of Law (HUDERIA Methodology)



用德语将"HUDERIA METHODOLOGY"改写成首字母大写的表达
除了首字母大写，还有哪些常见的改写文本格式？
怎样确保改写后的文本准确传达原意？.pdf)] | 标准指南、技术性文件 | 🇪🇺 欧盟 | 2024-09-28 00:00:00 | CAI, Committee on Artificial Intelligence | https://www.coe.int/en/web/artificial-intelligence/huderia-risk-and-impact-assessment-of-ai-systems | 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 6. 模型 - 可解释性 , 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全）, 9. 模型 - 准确性、可靠性（幻觉）, 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为） | AI 生命周期 |
| [[Open letter to UK online service providers regarding Generative AI and chatbots](data/Open letter to UK online service providers regarding Generative AI and chatbots.pdf)] | 标准指南、技术性文件 | 🇬🇧 英国 | 2024-11-08 00:00:00 | OFCOM | https://www.ofcom.org.uk/online-safety/illegal-and-harmful-content/open-letter-to-uk-online-service-providers-regarding-generative-ai-and-chatbots/ | 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全）, 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为） | AI 应用场景 |
| [[Introduction to AI Assurance](data/Introduction to AI Assurance.pdf)] | 标准指南、技术性文件 | 🇬🇧 英国 | 2024-02-12 00:00:00 | DSIT, Department for Science, Innovation and Technology | https://www.gov.uk/government/publications/introduction-to-ai-assurance | 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 7. 模型 - 公平性（算法涉及层面）, 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全）, 9. 模型 - 准确性、可靠性（幻觉）, 13. 生成内容 - 数据泄露（包括隐私泄露）, 2. 训练数据 - 违规收集 - 个人信息相关 | AI 生命周期, AI 应用场景 |
| [[Guidance for using the AI Management Essentials tool](data/Guidance for using the AI Management Essentials tool.pdf)] | 标准指南、技术性文件 | 🇬🇧 英国 | 2024-11-06 00:00:00 | DSIT | https://www.gov.uk/government/consultations/ai-management-essentials-tool/guidance-for-using-the-ai-management-essentials-tool | 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 7. 模型 - 公平性（算法涉及层面）, 9. 模型 - 准确性、可靠性（幻觉）, 5. 训练数据 - 数据泄露, 13. 生成内容 - 数据泄露（包括隐私泄露）, 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全） | AI 生命周期 |
| [[Compliance of products with embedded artificial intelligence](data/Compliance of products with embedded artificial intelligence.pdf)] | 标准指南、技术性文件 | 🌍 NONE | 2024-11-04 00:00:00 | UNECE，联合国欧洲经济委员会 | https://unece.org/trade/publications/ece_trade_486 | 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 9. 模型 - 准确性、可靠性（幻觉）, 8. 模型 - 鲁棒性、抗干扰性, 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全）, 18. 供应链相关风险 | AI 应用场景 |
| [[Model AI Governance Framework for Generative AI](data/Model AI Governance Framework for Generative AI.pdf)] | 标准指南、技术性文件 | 🇸🇬 新加坡 | 2024-05-24 00:00:00 | IMDA | https://aiverifyfoundation.sg/resources/mgf-gen-ai/ | 1. 训练数据 - 违规收集 - 知产相关, 2. 训练数据 - 违规收集 - 个人信息相关, 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 6. 模型 - 可解释性 , 9. 模型 - 准确性、可靠性（幻觉）, 10. 模型 - 被篡改风险（如模型参数被篡改）, 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全）, 14. 生成内容 - 模型幻觉, 18. 供应链相关风险 | AI 生命周期 |
| [[AI and cyber security: what you need to know](data/AI and cyber security: what you need to know.pdf)] | 研究报告 | 🇬🇧 英国 | 2024-02-13 00:00:00 | NCSC | https://www.ncsc.gov.uk/guidance/ai-and-cyber-security-what-you-need-to-know | 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 9. 模型 - 准确性、可靠性（幻觉）, 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全）, 14. 生成内容 - 模型幻觉, 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为） | AI 风险来源 |
| [[Machine learning principles](data/Machine learning principles.pdf)] | 标准指南、技术性文件 | 🇬🇧 英国 | 2024-05-22 00:00:00 | NCSC | https://www.ncsc.gov.uk/collection/machine-learning-principles | 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 8. 模型 - 鲁棒性、抗干扰性, 10. 模型 - 被篡改风险（如模型参数被篡改）, 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全）, 13. 生成内容 - 数据泄露（包括隐私泄露）, 18. 供应链相关风险, 19. 系统相关风险（api、交互页面等） | AI 生命周期 |
| [[The Artificial Intelligence and Data Act (AIDA)  – Companion document](data/The Artificial Intelligence and Data Act (AIDA)  – Companion document.pdf)] | 标准指南、技术性文件 | 🇨🇦 加拿大 | 2023-03-13 00:00:00 | Minister of Innovation, Science and Industry | https://ised-isde.canada.ca/site/innovation-better-canada/en/artificial-intelligence-and-data-act-aida-companion-document | 2. 训练数据 - 违规收集 - 个人信息相关, 4. 训练数据 - 内容不当（歧视偏见、不具有多样性、被投毒等）, 7. 模型 - 公平性（算法涉及层面）, 8. 模型 - 鲁棒性、抗干扰性, 9. 模型 - 准确性、可靠性（幻觉）, 12. 生成内容 - 内容安全风险（如歧视、偏见、不符合伦理道德、危害社会公共利益、国家安全）, 13. 生成内容 - 数据泄露（包括隐私泄露）, 15. 生成内容 - 不当使用导致风险（各种恶意攻击、各种将AI生成结果用户非法活动的行为）, 19. 系统相关风险（api、交互页面等） | AI 生命周期, AI 风险来源 |
| [[ Companion Guide on Securing AI Systems](data/ Companion Guide on Securing AI Systems.pdf)] | 标准指南、技术性文件 | 🇸🇬 新加坡 | 2024-10-15 00:00:00 | CSA | https://www.csa.gov.sg/Tips-Resource/publications/2024/guidelines-on-securing-ai | nan | nan |
| [[Online Safety Act](data/Online Safety Act.pdf)] | 法律法规 - 已生效 | 🇬🇧 英国 | 2023-10-26 00:00:00 | OFCOM | https://www.legislation.gov.uk/ukpga/2023/50/section/1/enacted | nan | nan |
